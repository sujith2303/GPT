{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujith2303/GPT/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "T598Uozsf81O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6WsqgRUEr32",
        "outputId": "32e1f74e-9b44-4277-fa51-f98f70fcb4f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7eecf01404d0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhEArrrqE6eD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zc1wl6L_sics"
      },
      "outputs": [],
      "source": [
        "batch_size = 1024 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 50000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "print_steps = 1000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 100\n",
        "head_dim = 64\n",
        "num_heads = 6\n",
        "n_layer = 1\n",
        "bias = False\n",
        "vocab_size = 65\n",
        "kwargs = {\n",
        "    \"dropout\": 0.2,\n",
        "    \"norm_type\":\"pre\",\n",
        "    \"linear_dropout\":0.2,\n",
        "    \"head_dropout\": 0.2,\n",
        "    \"block_size\" : 256\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wJoUGG4XgGWr"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim, bias = True, device = \"cpu\",*args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.query   = nn.Linear(embed_dim, head_dim, bias= bias, device=device)\n",
        "        self.key     = nn.Linear(embed_dim, head_dim, bias= bias, device=device)\n",
        "        self.value   = nn.Linear(embed_dim, head_dim, bias= bias, device=device)\n",
        "        self.dropout = nn.Dropout(kwargs.get(\"head_dropout\",0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C =  x.shape\n",
        "        device = x.device\n",
        "        tril =  torch.tril(torch.ones(x.shape[1],x.shape[1])).to(device) # Move tril to the same device as x\n",
        "\n",
        "        q = self.query(x)   ## B T H\n",
        "        k = self.key(x)     ## B T H\n",
        "        v = self.value(x)   ## B T H\n",
        "\n",
        "        matmul =  q @ k.transpose(-2,-1)  * k.shape[-1]**0.5 ## B T H   B H T  = B T T\n",
        "\n",
        "        wei = matmul.masked_fill(tril == 0, float('-inf'))  # (B, T, T)\n",
        "\n",
        "        wei  = F.softmax(wei, dim = -1)\n",
        "        attn_score = wei\n",
        "        wei  = self.dropout(wei)\n",
        "\n",
        "        wei = wei @ v ## B T T ->   B T C       == B T C\n",
        "        return wei,attn_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "g5nAIUhXYSFw"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads, bias = False, device = \"cpu\",*args, **kwargs):\n",
        "        super().__init__()\n",
        "        embed_dim = head_dim * num_heads\n",
        "        self.multiheads = nn.ModuleList([Head(embed_dim, head_dim, bias=bias, device = device, *args, **kwargs) for _ in range(num_heads)])\n",
        "        self.out = nn.Linear(embed_dim, embed_dim,device = device)\n",
        "        self.dropout = nn.Dropout(kwargs.get(\"dropout\",0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x)[0] for h in self.multiheads], dim=-1)\n",
        "        out = self.dropout(self.out(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bwUZ0EL4hohm"
      },
      "outputs": [],
      "source": [
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, bias = False, device = \"cpu\",*args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4*embed_dim,device = device),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*embed_dim, embed_dim, device = device),\n",
        "            nn.Dropout(kwargs.get(\"linear_dropout\",0.2))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S2vtI4XbgQHZ"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads, bias = False, device = \"cpu\",*args, **kwargs):\n",
        "        super().__init__()\n",
        "        embed_dim = num_heads*head_dim\n",
        "        self.mha    = MultiHeadAttention(head_dim=head_dim, num_heads = num_heads, bias = bias, device =device,*args,**kwargs)\n",
        "        self.linear = LinearBlock(embed_dim = embed_dim, bias= bias,device = device, *args, **kwargs)\n",
        "        self.norm1  = nn.LayerNorm(embed_dim, device = device)\n",
        "        self.norm2  = nn.LayerNorm(embed_dim, device=device)\n",
        "        self.norm_type = kwargs.get(\"norm_type\",\"pre\")\n",
        "    def forward(self, x):\n",
        "        if self.norm_type==\"pre\":\n",
        "            x = self.mha(self.norm1(x))+x  ## pre norm\n",
        "            x = self.linear(self.norm2(x))+x\n",
        "        else:\n",
        "            x = self.norm1(self.mha(x) + x)\n",
        "            x = self.norm2(self.linear(x)+x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1FqVoizUiRIH"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers, head_dim, num_heads, bias = False,device = \"cpu\", *args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        embed_dim = head_dim * num_heads\n",
        "        self.block_size = kwargs.get(\"block_size\",256)\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embed_dim)\n",
        "        self.pos_embeddings = nn.Embedding(self.block_size, embedding_dim=embed_dim)\n",
        "        self.blocks = nn.Sequential(*[Block(head_dim=head_dim, num_heads=num_heads,bias = bias, device = device, *args, **kwargs) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "        self.out = nn.Linear(embed_dim,vocab_size, device = device)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim,vocab_size)\n",
        "\n",
        "    def forward(self, x, targets = None):\n",
        "        B,T = x.shape\n",
        "        device = x.device\n",
        "        x = self.embeddings(x).to(device) + self.pos_embeddings(torch.arange(T, device=device))\n",
        "        x = self.blocks(x).to(device)\n",
        "        x = self.norm3(x).to(device)\n",
        "        logits = self.out(x).to(device)\n",
        "\n",
        "        if targets==None:\n",
        "            loss =  None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            x = x[:,-self.block_size:]\n",
        "            logits, _ = self(x)\n",
        "            B, T, C  = logits.shape\n",
        "            logits   = logits[:,-1,:]\n",
        "            probs    = F.softmax(logits,dim=-1)\n",
        "            next_idx = torch.multinomial(probs, num_samples = 1)\n",
        "            x = torch.cat((x,next_idx), dim = 1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Zys7b8eAMEAH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "FE-T5at54q_i"
      },
      "outputs": [],
      "source": [
        "model = GPTLanguageModel(vocab_size=vocab_size,\n",
        "                         num_layers=  n_layer,\n",
        "                         head_dim = head_dim,\n",
        "                         num_heads = num_heads,\n",
        "                         bias = bias,\n",
        "                         **kwargs).to(device) # Move the model to the correct device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SONGCcOzFk8x",
        "outputId": "4c4ad541-c541-4a41-fcf0-23c4ec379aa0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (embeddings): Embedding(65, 384)\n",
              "  (pos_embeddings): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (multiheads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (out): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (linear): LinearBlock(\n",
              "        (linear): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=384, out_features=65, bias=True)\n",
              "  (norm3): LayerNorm((384,), eps=65, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v50vdwjXArNv"
      },
      "source": [
        "## Load text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0vGg-3yDPYK",
        "outputId": "741e2642-27fc-4d39-8878-e2ce5ee3bcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-18 13:50:50--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-18 13:50:51 (24.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "dZMQWQuIAsS1"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split==\"train\" else val_data # Use val_data for 'val' split\n",
        "    ix  = torch.randint(len(data)-block_size,(batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Xocn5HyqCO6Z"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GTD2_4sMjdN",
        "outputId": "3568e992-4678-434a-c495-760b9496e389"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor(4.1883), 'val': tensor(4.1871)}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "estimate_loss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "DRT_g6AJBCEG"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYj2Obd-B445",
        "outputId": "348fac63-6d0f-4f5f-d0e8-cb5742c8a621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1883, val loss 4.1871\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            "oy xUlhwnuE!OkTenS:N\n",
            "v3&KfcFIGDpEcfDMLJcm!&$3KxTBf.IXAZriJCKSrWDi m.fBEHjG.GdAuUT3VSbhrYfXuWt$VU ktK&VuVEaYiWFIOY?mY3xspgW h-d&DprvRFP?Ba?pgcxH$Zl-Xe&gQlfe,KZkP,,3PH&byytauL $IJGowRghRZJWU':mQxUxaNPqnGtqlMq lzz!PRB&,OcP,rKm?TzaTJmmETvi!,,K\n",
            "yBi,\n",
            "Vb\n",
            "jZbBw r-I\n",
            "\n",
            "\n",
            "\n",
            "step 500: train loss 1.9327, val loss 2.0487\n",
            "step 1000: train loss 1.5270, val loss 1.7196\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            "u.\n",
            "\n",
            "Patreak peak Telant, a confisl hands pont my know mire?\n",
            "For the all of York,\n",
            "Go outhe Friator:\n",
            "Rethat it onder theehithere full sher hast iain\n",
            "The ane marreof thou selt,\n",
            "And on to the Angely fresweat wosserveong of ve age:\n",
            "And mot; the cripessenced,\n",
            "And\n",
            "\n",
            "\n",
            "\n",
            "step 1500: train loss 1.4527, val loss 1.6634\n",
            "step 2000: train loss 1.4251, val loss 1.6468\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            "bod-s hare fathere.\n",
            "Sake poor:\n",
            "Farewell.\n",
            "Vour per:\n",
            "'Ay worn sets'\n",
            "\n",
            "RICHARD:\n",
            "Terchers a\n",
            "lowers, it name tater of BUaugs as not:\n",
            "Misport o'rt will'd and tell; foot with alway than is wife!\n",
            "Are you were intell s' like a wore than my look'd the suppliance!\n",
            "Wher\n",
            "\n",
            "\n",
            "\n",
            "step 2500: train loss 1.4044, val loss 1.6377\n",
            "step 3000: train loss 1.3931, val loss 1.6329\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            " are of myself aling wome,\n",
            "Thou noble he simps his spirawn hath\n",
            "i' the to bid\n",
            "tet exes, died with lidings, youngeance:\n",
            "Let me prone you!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "I'll weep gently thou, in prifaired\n",
            "To a leve a fren for deedy brought.\n",
            "\n",
            "GRFOMEO:\n",
            "\n",
            "BUCKING RICHARD III:\n",
            "\n",
            "\n",
            "\n",
            "step 3500: train loss 1.3837, val loss 1.6285\n"
          ]
        }
      ],
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval==0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    if iter % print_steps ==0:\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(\"\\n\\nPREDICTIONS\")\n",
        "        print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "        print(\"\\n\\n\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF97cjk_DMGJ",
        "outputId": "e30641f4-4490-454d-841b-0aaa857afe11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JhhXb\n",
            ":hJCvXiKKwZxxSMyv.Upm'XwoO :EJXd\n",
            "MC!HbH&eJuVLSImJ\n",
            "fX f,L$J.kqfmV.eaV&Ki;W!vKY\n",
            "\n",
            "uVmOpuTk-IkdINQtH3sb!EjToTisHD GcJyW bGAm$WdIKaPMV .' ywu'UNjtgoi&q?YyosdU?-YDNOLc,jsUJ:HNFdhkg.tC-zGQjK3Z&:jgdab,dssL.MZaBVEUabnAXug Z:tCF tpuWYK.D3T'PwJWAtvv\n",
            ":MJDUl'ixDqI\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eFFOssKPu1p"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}