{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnyPhErXVFLpdiiyhGZOC4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujith2303/GPT/blob/main/GPT_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f00F2-sPE30B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51K_F-UBE7O5",
        "outputId": "1120e88c-3df3-4d4a-eadf-34bcaceb0671"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4dec6544d0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 50000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "print_steps = 1000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 100\n",
        "head_dim = 64\n",
        "num_heads = 6\n",
        "n_layer = 1\n",
        "bias = False\n",
        "vocab_size = 65\n",
        "kwargs = {\n",
        "    \"dropout\": 0.2,\n",
        "    \"norm_type\":\"post\",\n",
        "    \"linear_dropout\":0.2,\n",
        "    \"head_dropout\": 0.2,\n",
        "    \"block_size\" : 256\n",
        "    }"
      ],
      "metadata": {
        "id": "QUD47LDiE-UD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim, bias = True, device = \"cpu\",*args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.query   = nn.Linear(embed_dim, head_dim, bias= bias, device=device)\n",
        "        self.key     = nn.Linear(embed_dim, head_dim, bias= bias, device=device)\n",
        "        self.value   = nn.Linear(embed_dim, head_dim, bias= bias, device=device)\n",
        "        self.dropout = nn.Dropout(kwargs.get(\"head_dropout\",0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C =  x.shape\n",
        "        device = x.device\n",
        "        tril =  torch.tril(torch.ones(x.shape[1],x.shape[1])).to(device) # Move tril to the same device as x\n",
        "\n",
        "        q = self.query(x)   ## B T H\n",
        "        k = self.key(x)     ## B T H\n",
        "        v = self.value(x)   ## B T H\n",
        "\n",
        "        matmul =  q @ k.transpose(-2,-1)  * k.shape[-1]**0.5 ## B T H   B H T  = B T T\n",
        "\n",
        "        wei = matmul.masked_fill(tril == 0, float('-inf'))  # (B, T, T)\n",
        "\n",
        "        wei  = F.softmax(wei, dim = -1)\n",
        "        attn_score = wei\n",
        "        wei  = self.dropout(wei)\n",
        "\n",
        "        wei = wei @ v ## B T T ->   B T C       == B T C\n",
        "        return wei,attn_score"
      ],
      "metadata": {
        "id": "rN9JWcTeFVz9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads, bias = False, device = \"cpu\",*args, **kwargs):\n",
        "        super().__init__()\n",
        "        embed_dim = head_dim * num_heads\n",
        "        self.multiheads = nn.ModuleList([Head(embed_dim, head_dim, bias=bias, device = device, *args, **kwargs) for _ in range(num_heads)])\n",
        "        self.out = nn.Linear(embed_dim, embed_dim,device = device)\n",
        "        self.dropout = nn.Dropout(kwargs.get(\"dropout\",0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x)[0] for h in self.multiheads], dim=-1)\n",
        "        out = self.dropout(self.out(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "oSTow4pTFW6O"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, bias = False, device = \"cpu\",*args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4*embed_dim,device = device),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*embed_dim, embed_dim, device = device),\n",
        "            nn.Dropout(kwargs.get(\"linear_dropout\",0.2))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "metadata": {
        "id": "zMgq_h1DFX2i"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, head_dim, num_heads, bias = False, device = \"cpu\",*args, **kwargs):\n",
        "        super().__init__()\n",
        "        embed_dim = num_heads*head_dim\n",
        "        self.mha    = MultiHeadAttention(head_dim=head_dim, num_heads = num_heads, bias = bias, device =device,*args,**kwargs)\n",
        "        self.linear = LinearBlock(embed_dim = embed_dim, bias= bias,device = device, *args, **kwargs)\n",
        "        self.norm1  = nn.LayerNorm(embed_dim, device = device)\n",
        "        self.norm2  = nn.LayerNorm(embed_dim, device=device)\n",
        "        self.norm_type = kwargs.get(\"norm_type\",\"pre\")\n",
        "    def forward(self, x):\n",
        "        if self.norm_type==\"pre\":\n",
        "            x = self.mha(self.norm1(x))+x  ## pre norm\n",
        "            x = self.linear(self.norm2(x))+x\n",
        "        else:\n",
        "            x = self.norm1(self.mha(x) + x)\n",
        "            x = self.norm2(self.linear(x)+x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VDUjJG5WFaXh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_layers, head_dim, num_heads, bias = False,device = \"cpu\", *args, **kwargs) -> None:\n",
        "        super().__init__()\n",
        "        embed_dim = head_dim * num_heads\n",
        "        self.block_size = kwargs.get(\"block_size\",256)\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim=embed_dim)\n",
        "        self.pos_embeddings = nn.Embedding(self.block_size, embedding_dim=embed_dim)\n",
        "        self.blocks = nn.Sequential(*[Block(head_dim=head_dim, num_heads=num_heads,bias = bias, device = device, *args, **kwargs) for _ in range(num_layers)])\n",
        "        self.num_layers = num_layers\n",
        "        self.out = nn.Linear(embed_dim,vocab_size, device = device)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim,vocab_size)\n",
        "\n",
        "    def forward(self, x, targets = None):\n",
        "        B,T = x.shape\n",
        "        device = x.device\n",
        "        x = self.embeddings(x).to(device) + self.pos_embeddings(torch.arange(T, device=device))\n",
        "        x = self.blocks(x).to(device)\n",
        "        x = self.norm3(x).to(device)\n",
        "        logits = self.out(x).to(device)\n",
        "\n",
        "        if targets==None:\n",
        "            loss =  None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            x = x[:,-self.block_size:]\n",
        "            logits, _ = self(x)\n",
        "            B, T, C  = logits.shape\n",
        "            logits   = logits[:,-1,:]\n",
        "            probs    = F.softmax(logits,dim=-1)\n",
        "            next_idx = torch.multinomial(probs, num_samples = 1)\n",
        "            x = torch.cat((x,next_idx), dim = 1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8lsV1l3OFbVp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uwzZea34Fdks"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel(vocab_size=vocab_size,\n",
        "                         num_layers=  n_layer,\n",
        "                         head_dim = head_dim,\n",
        "                         num_heads = num_heads,\n",
        "                         bias = bias,\n",
        "                         **kwargs).to(device) # Move the model to the correct device"
      ],
      "metadata": {
        "id": "BmDB6T9GFdm2"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POiyoPw7Fotk",
        "outputId": "ef00c56c-5518-48c0-a8ae-51b8c5cee3eb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (embeddings): Embedding(65, 384)\n",
              "  (pos_embeddings): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (mha): MultiHeadAttention(\n",
              "        (multiheads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (out): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (linear): LinearBlock(\n",
              "        (linear): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=384, out_features=65, bias=True)\n",
              "  (norm3): LayerNorm((384,), eps=65, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aeA_JMrFFpBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loadnig"
      ],
      "metadata": {
        "id": "Cu8BS2JMGus3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFWLdZXYGv6T",
        "outputId": "0d52c412-22b1-4ef5-a78e-9845b5056e0f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-18 13:54:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-09-18 13:54:36 (33.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "    data = train_data if split==\"train\" else val_data # Use val_data for 'val' split\n",
        "    ix  = torch.randint(len(data)-block_size,(batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "Y6qGnYWcGwMq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "9aCP7sD2GxfI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimate_loss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8Z1ZW-rGyQU",
        "outputId": "db28e3b0-c81e-4c8f-a055-ee04017c4056"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor(4.1833), 'val': tensor(4.1824)}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters())"
      ],
      "metadata": {
        "id": "619k5ZaXGzJ5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval==0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    if iter % print_steps ==0:\n",
        "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "        print(\"\\n\\nPREDICTIONS\")\n",
        "        print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "        print(\"\\n\\n\")\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhpfraPgG0Vb",
        "outputId": "7e1c10b1-9eaf-428f-c00f-92b1d7ebc84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1833, val loss 4.1824\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            "oy xUlhwnuE!OkTenS:N\n",
            "v3&KfcFIGDpEcfDMLJcm!&$3KxTBf.IXAZriJCKsrWDi m.fBE$jG.GdAuUT3VSbhrYfXuWt$VU ktK&VuVEaYiWFIOY?mY3xspgW h-dRjprvRFP?Ba?pgcxH$Zl-Xe&gQlfe,KZkP,,3PH&byytauL $IJGowRghRZJWU':LQxUxwAPqnGtqKMq lzz!PRB&,OcP,rKm?TzETJmmETvi!,,K\n",
            "yBi,\n",
            "Vb\n",
            "jZbBw r-I\n",
            "\n",
            "\n",
            "\n",
            "step 500: train loss 2.0577, val loss 2.1399\n",
            "step 1000: train loss 1.6544, val loss 1.8377\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            "v.\n",
            "\n",
            "MENrth, peaitie avy, and cous: hicharppet of kyou misesemeter deall ot pet\n",
            "ays souch as its rusthat your ond out tevith rell thes pracasiniand therane allmany tead st tark, mon prot\n",
            "Morcue hofres' thesossed! ond banve aglly woll welf mocrt mise as onded\n",
            "\n",
            "\n",
            "\n",
            "step 1500: train loss 1.6164, val loss 1.8152\n",
            "step 2000: train loss 1.6017, val loss 1.8063\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            " often,\n",
            "And methot.\n",
            "Alone of our ring thince make our aworness ally comp, I ant Rome tely the with t well hre thou tosere, I by tren\n",
            "He true,\n",
            "We'll do thathter'd ay.\n",
            "O do mond of hat youl repalad prought tres' mind by thers fere'stick'd the it atireseerock \n",
            "\n",
            "\n",
            "\n",
            "step 2500: train loss 1.5925, val loss 1.8027\n",
            "step 3000: train loss 1.5860, val loss 1.7976\n",
            "\n",
            "\n",
            "PREDICTIONS\n",
            "Harklood Atrid genry wome,\n",
            "The malimal schief\n",
            "What spirawn hat ith thou call foet exechard bent me? done owe, herab gir, Which son you;\n",
            "And by thant:\n",
            "He'd 'ther boor in my hon, in arthe pedlind yersee hat hofolld thy brince, is hopest you have thread,\n",
            "What \n",
            "\n",
            "\n",
            "\n",
            "step 3500: train loss 1.5806, val loss 1.7948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19SGkNmnG1Kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}